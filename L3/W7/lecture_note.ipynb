{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundation models (Optional Content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Foundation Models\n",
    "\n",
    "#### Introduction\n",
    "- New module on foundation models and generative AI in healthcare.\n",
    "- Focus on the impact of foundation models like ChatGPT and DALL-E.\n",
    "\n",
    "#### Key Concepts\n",
    "1. **Foundation Models**:\n",
    "   - Popularized by Stanford, foundation models are a type of AI model that:\n",
    "     - Learn from massive amounts of unlabeled data (unsupervised learning).\n",
    "     - Use self-supervised learning techniques.\n",
    "     - Exhibit adaptability and improved sample efficiency as data size and parameters increase.\n",
    "\n",
    "2. **Model Size**:\n",
    "   - \"Large\" in large language models refers to the number of parameters:\n",
    "     - GPT: 117 million\n",
    "     - GPT-2: 1.5 billion\n",
    "     - GPT-3: 175 billion (trained on 500 billion word tokens).\n",
    "   - Similar architectures but with increasing size and data.\n",
    "\n",
    "3. **Learning Techniques**:\n",
    "   - **Few-shot Learning**: Model learns from a small number of examples and generalizes well.\n",
    "   - **Zero-shot Learning**: Model performs new tasks it has never seen before using general knowledge.\n",
    "   - Both are aspects of **Transfer Learning**.\n",
    "\n",
    "#### Applications in Healthcare\n",
    "- Foundation models can handle multiple data types (multimodal) and tasks, such as:\n",
    "  - Chatbots\n",
    "  - Language-based search engines\n",
    "  - Image creation\n",
    "  - Automated content generation.\n",
    "- Benefits include:\n",
    "  - Efficient data processing and understanding.\n",
    "  - Improved patient care and outcomes.\n",
    "  - Cost-effective model development and maintenance.\n",
    "\n",
    "#### Analogy\n",
    "- Chef analogy for understanding:\n",
    "  - Chef trained broadly can quickly adapt to new recipes (few-shot learning).\n",
    "  - Chef can recreate unfamiliar dishes based on general cooking knowledge (zero-shot learning).\n",
    "\n",
    "#### Potential Impact\n",
    "- Foundation models can transform healthcare with applications in:\n",
    "  - Automated screening\n",
    "  - Diagnosis\n",
    "  - Drug design.\n",
    "- Enhancements include better communication between healthcare professionals and AI systems, leading to:\n",
    "  - Efficient diagnoses and treatments.\n",
    "  - Improved workflow and collaboration.\n",
    "\n",
    "#### Conclusion\n",
    "- Foundation models bridge the gap in healthcare AI, promoting improved clinical outcomes and patient lives.\n",
    "- Importance for stakeholders in healthcare to understand and utilize foundation models, making the course accessible to all backgrounds.\n",
    "\n",
    "#### Key Takeaway\n",
    "- Foundation models are reshaping AI applications, particularly in healthcare, with their adaptability, efficiency, and multimodal capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapting to Technology\n",
    "\n",
    "#### Exponential Growth in Technology\n",
    "- The rapid advancements in AI, particularly foundation models, represent a shift from academic concepts to transformative forces in industries.\n",
    "- Exponential phenomena are often counterintuitive in a linear world, making it difficult for humans to adapt and plan.\n",
    "\n",
    "#### The Chessboard Analogy\n",
    "- The story of the chessboard illustrates exponential growth:\n",
    "  - An inventor requests rice grains doubling on each square of a chessboard, highlighting how quickly quantities can escalate.\n",
    "  - By the 64th square, the total grains far exceed what exists in the world, showing the underestimation of exponential growth's impact.\n",
    "\n",
    "#### Data and Computing Power\n",
    "- The growth of data and computational power fuels AI development:\n",
    "  - Exponential increase in healthcare data and a reduction in the doubling time for healthcare knowledge challenge healthcare systems and professionals.\n",
    "  \n",
    "#### Moore's Law\n",
    "- Gordon Mooreâ€™s observation states that the number of transistors on microchips doubles roughly every two years, driving rapid technological advancement.\n",
    "- Example:\n",
    "  - In 2000: ~2 million transistors per chip.\n",
    "  - In 2020: ~100 billion transistors, a 50,000-fold increase.\n",
    "  - By 2040: projected ~1 trillion transistors, a 50 million-fold increase over 40 years.\n",
    "\n",
    "#### Challenges and Implications\n",
    "- The exponential growth presents challenges for planning, governance, and ethical considerations:\n",
    "  - Society struggles to anticipate the implications of rapid advancements.\n",
    "  - The need for responsible innovation and ethical frameworks is paramount.\n",
    "\n",
    "#### Foundation Models and Their Tipping Point\n",
    "- Foundation models may represent a tipping point in AI, paralleling the chessboard's second half.\n",
    "- Critical questions arise about their impact on healthcare and society.\n",
    "\n",
    "#### Best Practices for Adaptation\n",
    "1. **Invest in R&D**: To prepare for the impact and leverage opportunities from exponential technologies.\n",
    "2. **Create Regulatory Frameworks**: Governments should ensure responsible development and usage of technologies, holding parties accountable.\n",
    "3. **Encourage Ethical Considerations**: Stakeholders must consider the social implications of their work.\n",
    "4. **Foster Cross-disciplinary Collaboration**: Diverse expertise is essential for addressing the challenges and opportunities of these technologies.\n",
    "5. **Prioritize Education**: Educating all stakeholders about AI and its implications is crucial for informed decision-making.\n",
    "\n",
    "#### Conclusion\n",
    "- Understanding and preparing for exponential technologies like AI is vital as they continue to reshape industries, especially healthcare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General AI and Emergent Behavior\n",
    "\n",
    "\n",
    "#### Transformative Potential of Foundation Models\n",
    "- Foundation models have sparked discussions about their societal impact, similar to past sentiments about computers.\n",
    "- These models have demonstrated impressive capabilities, such as passing rigorous tests in law, business, and medicine, leading to debates about general AI.\n",
    "\n",
    "#### Understanding General AI\n",
    "- General AI is interpreted in various ways, often surrounded by hype and confusion.\n",
    "- Foundation models exemplify a significant advancement in knowledge-based tasks and can be viewed as a form of general AI.\n",
    "\n",
    "#### Learning Mechanisms\n",
    "- Foundation models leverage unsupervised learning from large datasets, enabling efficiencies in tasks like transfer learning, few-shot, and zero-shot learning.\n",
    "\n",
    "#### Emergent Behaviors\n",
    "- Emergent behaviors arise as foundation models, with increasing parameters and data, exhibit unexpected capabilities not explicitly programmed:\n",
    "  - These behaviors can be beneficial (e.g., discovering new patterns) or problematic (e.g., biased outputs).\n",
    "- Understanding how these behaviors emerge is crucial for effectively working with foundation models.\n",
    "\n",
    "#### Analogy of Flocking Birds\n",
    "- Similar to how individual birds coordinate in a flock, foundation models find patterns through complex interrelationships within massive datasets, leading to new capabilities.\n",
    "\n",
    "#### Healthcare Applications\n",
    "- In healthcare, emergent behaviors can uncover connections between symptoms, conditions, and treatments, potentially improving diagnostics and patient outcomes.\n",
    "- They also hold promise for drug discovery by analyzing diverse data sources to identify new drug targets and interactions.\n",
    "\n",
    "#### Challenges of Emergent Behaviors\n",
    "- Understanding the complex relationships revealed by foundation models can be difficult, raising issues of transparency and reliability.\n",
    "- The \"black box\" phenomenon complicates understanding how models arrive at conclusions.\n",
    "\n",
    "#### Hallucinations\n",
    "- Hallucinations refer to false or unrealistic outputs generated by the model, often when the input falls outside the training data.\n",
    "- In healthcare, this can result in incorrect recommendations or treatment suggestions, posing risks to patient safety.\n",
    "\n",
    "#### Addressing Challenges\n",
    "- To mitigate hallucinations, it's essential to:\n",
    "  - Train models on diverse, representative datasets.\n",
    "  - Validate predictions through rigorous testing.\n",
    "  - Enhance model transparency and interpretability.\n",
    "  \n",
    "- An innovative approach involves using AI to monitor AI, comparing outputs to validated sources to identify inaccuracies.\n",
    "\n",
    "#### Conclusion\n",
    "- The challenges posed by foundation models and emergent behaviors require ongoing research and focus on specific domains like healthcare.\n",
    "- While complex, strategies such as AI monitoring can help address issues of false outputs, paving the way for responsible and effective use of foundation models in real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Foundation Models Work\n",
    "\n",
    "### Key Steps in Training a Foundation Model\n",
    "\n",
    "1. **Data Collection**:\n",
    "   - Large volumes of text data are gathered from various sources (books, articles, websites) to create a diverse training set.\n",
    "\n",
    "2. **Preprocessing**:\n",
    "   - The text is tokenized, breaking it down into words or sub-words. This step is crucial as it transforms raw text into a format that the model can process.\n",
    "   - Additional preprocessing steps may include cleaning the data, removing irrelevant content, and normalizing text.\n",
    "\n",
    "3. **Model Architecture Design**:\n",
    "   - The backbone of most LLMs is the **transformer architecture**, known for its self-attention mechanism that allows the model to weigh the importance of different words in a sentence.\n",
    "   - There are two primary types of transformer architectures:\n",
    "     - **Transformer Encoder**: Processes input sequences and captures semantic meaning.\n",
    "     - **Transformer Decoder**: Generates output sequences based on an initial vector or input.\n",
    "\n",
    "4. **Training**:\n",
    "   - LLMs typically use **self-supervised learning**. For instance, BERT employs masked language modeling where random tokens are masked, and the model learns to predict them using surrounding context.\n",
    "   - The GPT series predicts the next word in a sequence based on previous words, training the model through exposure to vast text corpora.\n",
    "\n",
    "5. **Evaluation**:\n",
    "   - After training, the model is evaluated on specific tasks to assess its performance and fine-tune it if necessary.\n",
    "   - Evaluation may involve metrics like perplexity for language models or specific benchmarks in the case of downstream tasks.\n",
    "\n",
    "### Prompt Engineering\n",
    "\n",
    "- **Prompts**: The input given to the model to generate output. It can be a question, instruction, or statement.\n",
    "- **Techniques**:\n",
    "  - **Instruction Prompt**: A simple command (e.g., \"What are the symptoms of flu?\").\n",
    "  - **Role Assignment**: Assigning a role (e.g., \"You are a doctor.\").\n",
    "  - **Few-Shot Prompting**: Providing examples to guide the modelâ€™s output.\n",
    "  - **Chain of Thought Prompting**: Encouraging the model to explain its reasoning process step-by-step.\n",
    "  - **Self-Consistency**: Generating multiple responses and choosing the most frequent answer.\n",
    "  - **Generative Knowledge**: Generating and integrating facts before producing a final response.\n",
    "\n",
    "### Multimodal Foundation Models\n",
    "\n",
    "- These models process different data types, such as text and images, using similar transformer architectures.\n",
    "- **Examples**:\n",
    "  - **DALLÂ·E**: Generates images from textual descriptions by mapping text to visual features and decoding them into images.\n",
    "  - **Whisper**: A model for speech recognition that encodes audio into vectors for processing.\n",
    "\n",
    "### Applications in Healthcare\n",
    "\n",
    "Understanding how these models work, particularly in generating coherent and contextually relevant information, can be transformative in healthcare applications, such as patient management and diagnostic support. However, it's crucial to address limitations like potential biases in the data and the need for further alignment with human reasoning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Healthcare Use Cases for Text Data\n",
    "\n",
    "1. **Performance of ChatGPT**:\n",
    "   - ChatGPT passed the USMLE, showing close performance to expert physicians.\n",
    "   - Highlights potential and limitations of LLMs in healthcare.\n",
    "   - Outperforms smaller domain-specific models; useful for patient education and clinical question answering.\n",
    "\n",
    "2. **Integration into Medical Training**:\n",
    "   - Consideration for LLMs in licensing exams to reflect real-world clinical practice.\n",
    "   - Importance for healthcare professionals to understand LLMs' benefits and limitations.\n",
    "\n",
    "3. **Caution in Clinical Use**:\n",
    "   - Vigilance required in reviewing LLM outputs due to potential inaccuracies and fabrications.\n",
    "   - Importance of verifying medical literature references generated by LLMs.\n",
    "\n",
    "4. **Clerical Task Automation**:\n",
    "   - LLMs can automate scheduling, triaging patient requests, and improving inbox management.\n",
    "   - Helps reduce clinician burnout and enhances job satisfaction.\n",
    "\n",
    "5. **Data Processing and NLP Tasks**:\n",
    "   - **Tokenization**: Breaking down clinical text into manageable pieces for analysis.\n",
    "   - **Named Entity Recognition**: Identifying entities like drugs and diseases.\n",
    "   - **Negation Detection**: Understanding relationships and sentiment (e.g., \"no renal cell carcinoma present\").\n",
    "   - **Relation Extraction**: Identifying connections between entities (e.g., tests and conditions).\n",
    "   - **De-identification**: Masking patient information for privacy.\n",
    "\n",
    "6. **Foundation Models in Data Science**:\n",
    "   - Capable of performing NLP tasks with minimal training data.\n",
    "   - Enhance generalization across various clinical notes and health systems.\n",
    "\n",
    "7. **Advanced Applications**:\n",
    "   - **Clinical Decision Support**: Suggest treatment options, drug interactions, and best practices.\n",
    "   - **Clinical Trial Recruitment**: Assess patient eligibility and improve communication about trials.\n",
    "   - **Patient Communication**: Answering queries, generating reminders, and translating medical jargon.\n",
    "   - **Billing and Coding**: Assisting in accurate medical coding for billing purposes.\n",
    "   - **Public Health**: Monitoring outbreaks through data integration and analysis.\n",
    "\n",
    "8. **Genomic Applications**:\n",
    "   - Processing genomic data in text formats (e.g., FASTA).\n",
    "   - Identifying disease-related patterns and relationships through multimodal data analysis.\n",
    "   - Pharmacogenomics: Unlocking therapy responses and side effects.\n",
    "\n",
    "9. **Drug Discovery Enhancements**:\n",
    "   - Foundation models aid in virtual screening, lead optimization, toxicity prediction, and mechanism of action prediction.\n",
    "   - Streamlines drug development processes and improves accuracy of predictions.\n",
    "\n",
    "10. **Future Implications**:\n",
    "    - The role of foundation models in healthcare is rapidly expanding.\n",
    "    - Potential to revolutionize patient care, clinical processes, and drug discovery.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Healthcare Use Cases for Non-textual Unstructured Data\n",
    "\n",
    "1. **Broad Capabilities of Foundation Models**:\n",
    "   - Large language models (LLMs) have demonstrated effectiveness in healthcare tasks without modification, particularly in low-risk, repetitive clinical roles such as data management and administrative tasks.\n",
    "\n",
    "2. **Multi-Modal Learning**:\n",
    "   - Foundation models can process various data types (e.g., text, images, sound) and learn relationships across modalities, enhancing the extraction of valuable insights from unstructured healthcare data.\n",
    "   - For instance, image-text pairing during training helps models understand relationships between medical images (e.g., chest CTs) and accompanying textual descriptions.\n",
    "\n",
    "3. **Advancements in Medical Imaging**:\n",
    "   - Foundation models have the potential to reduce the need for numerous narrow models in medical imaging, improving efficiency in tasks like image quantification, detection, and risk prediction.\n",
    "   - Radiology, particularly neuro and chest radiology, has seen significant AI adoption, with radiologists expressing satisfaction with AI's added value to patient care.\n",
    "\n",
    "4. **Comprehensive Cognitive Tasks**:\n",
    "   - Radiologists' work involves comparing current exams with previous ones, synthesizing patient context, and making treatment recommendationsâ€”tasks that require understanding beyond just image interpretation.\n",
    "   - Foundation models could bridge this gap by integrating multiple data sources, allowing for a more holistic approach to diagnosis.\n",
    "\n",
    "5. **Enhanced Interpretations**:\n",
    "   - By combining image data with relevant clinical text, foundation models provide nuanced interpretations of medical images that consider the patient's medical history and treatment context.\n",
    "   - This broader understanding could lead to new AI models capable of extracting insights that human experts might miss.\n",
    "\n",
    "6. **Emerging Applications**:\n",
    "   - Early studies suggest AI-derived measures from imaging can predict adverse events, facilitating screenings for various conditions like osteoporosis and cardiovascular disease.\n",
    "   - Multimodal foundation models may incorporate genomic and digital pathology data to discover new clinical patterns.\n",
    "\n",
    "7. **Streamlined Model Development**:\n",
    "   - Transfer learning allows for the rapid development of specific models with less labeled data, accelerating the advancement of medical imaging AI tools.\n",
    "   - These models can assist in tasks such as data preprocessing, augmentation, and generating synthetic imaging data.\n",
    "\n",
    "8. **Voice-Text Applications**:\n",
    "   - Foundation models can analyze voice data for disease prediction (e.g., detecting Parkinson's symptoms) and assist individuals with speech impairments.\n",
    "   - They also support virtual medical assistants and chatbots in mental health applications, enhancing access to care and information.\n",
    "\n",
    "9. **Augmenting Human Decision-Making**:\n",
    "   - Foundation models can transition humans from comprehension knowledge to fluid reasoning, augmenting problem-solving and decision-making capabilities in real-time.\n",
    "   - They are tools to support informed decision-making, not replacements for human judgment.\n",
    "\n",
    "10. **Caveats and Challenges**:\n",
    "    - Despite their capabilities, foundation models must adhere to the \"no free lunch\" theorem, indicating trade-offs and challenges exist in their implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges and Pitfalls\n",
    "\n",
    "This discussion highlights the vital intersection of technology and healthcare, emphasizing the necessity for technical literacy among medical professionals. As automation and technology progress rapidly, it's crucial for clinicians to not only understand medical knowledge but also how to effectively use advanced technologies like AI and machine learning.\n",
    "\n",
    "Key points include:\n",
    "\n",
    "1. **Technical Literacy**: Essential for navigating the growing complexity of healthcare technologies, especially as medical information doubles frequently. \n",
    "\n",
    "2. **Challenges of Machine Learning**: Issues such as bias, automation bias, and the risks of incorrect outputs pose significant challenges. Misleading outputs can lead to dangerous situations in patient care.\n",
    "\n",
    "3. **Foundation Models**: While powerful, they require careful governance to prevent issues stemming from training biases and incorrect human feedback. The potential for misinformation highlights the need for medical expertise in model development.\n",
    "\n",
    "4. **Resource Imbalances**: The high costs of developing foundation models raise concerns about disparities in access and quality of healthcare technologies across different populations.\n",
    "\n",
    "5. **Deployment Issues**: Integrating machine learning models into healthcare is complicated by fragmented IT systems, data silos, and strict privacy regulations.\n",
    "\n",
    "6. **Post-Deployment Monitoring**: Ongoing monitoring is critical to address model drift and ensure that models continue to perform effectively over time. \n",
    "\n",
    "7. **Opportunities for Improvement**: Despite challenges, the integration of technology in healthcare offers substantial potential for enhancing patient outcomes.\n",
    "\n",
    "By fostering technical skills and understanding the complexities of AI in healthcare, professionals can actively contribute to a patient-centered future while navigating the associated risks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
