{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias in AI Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "#### Key Concepts:\n",
    "1. **AI in Clinical Practice**: AI solutions are increasingly being integrated into healthcare, but concerns exist about their generalizability and fairness across diverse populations.\n",
    "\n",
    "2. **Generalization Issues**: Many AI models are trained on historical data from academic medical centers, which often lack representation of diverse populations. This creates models that may fail to demonstrate external validity when applied to broader patient communities.\n",
    "\n",
    "3. **Perpetuating Disparities**: AI systems trained on non-representative data risk reinforcing existing disparities, particularly in marginalized groups like African Americans in the USA.\n",
    "\n",
    "4. **Sources of Bias**: \n",
    "   - Training data may not represent all populations, leading to biased AI decisions.\n",
    "   - Poor understanding of how diseases manifest differently across populations.\n",
    "   - Limited awareness of potential biases in AI models.\n",
    "\n",
    "5. **Bias in Healthcare**: Health disparities based on demographics are more prevalent in countries like the USA, where access to care and outcomes differ significantly by race, ethnicity, and socioeconomic status. AI solutions could worsen these disparities if not carefully monitored.\n",
    "\n",
    "6. **Total Product Life Cycle of AI Solutions**: \n",
    "   - **Design and Development**: Involves creating the AI model.\n",
    "   - **Evaluation and Validation**: Ensuring that the AI performs accurately and fairly.\n",
    "   - **Deployment**: Introducing the AI model into healthcare settings.\n",
    "   - **Downstream Evaluation**: Ongoing monitoring of the AI's impact post-deployment, with a focus on ensuring equity across diverse populations.\n",
    "\n",
    "7. **Focus of the Lecture**: \n",
    "   - **Downstream Evaluation**: Monitoring the AI once deployed to ensure it produces equitable and unbiased outcomes.\n",
    "   - **Learning Objectives**:\n",
    "     - Overview of bias and fairness in AI healthcare solutions.\n",
    "     - Types of bias in AI models.\n",
    "     - Algorithmic fairness.\n",
    "     - Solutions to address bias in AI healthcare models.\n",
    "\n",
    "#### Key Points for Study:\n",
    "- The problem of **data representation** in AI training, especially with historical data.\n",
    "- **Disparities** in AI outcomes for non-white populations, specifically African Americans in the USA.\n",
    "- The **impact of demographic bias** in healthcare and the importance of fairness across all stages of AI model development.\n",
    "- The importance of **downstream evaluation** for ensuring equitable patient outcomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real World Examples of AI Bias\n",
    "\n",
    "#### Key Concepts:\n",
    "1. **Heart Failure and Gender Bias**:\n",
    "   - **Clinical Trials and Guidelines**: Historically, heart failure treatment guidelines were based on male symptoms, which differ from female symptoms.\n",
    "   - **Modeling Bias**: AI models trained on male heart attack symptoms are highly accurate for men but less accurate for women, leading to **gender bias** in predictive outcomes.\n",
    "   - **Framingham Study**: The study highlighted biases in cardiovascular disease treatment due to underrepresentation of womenâ€™s symptoms.\n",
    "\n",
    "2. **Genomic Databases and Racial Bias**:\n",
    "   - **Genomic Bias**: Publicly available genomic databases, which are foundational to precision medicine, are biased towards individuals of **European descent**.\n",
    "   - **Underrepresentation of Other Populations**: 81% of genome mapping participants in over 2,500 studies were of European descent, leading to **less accurate genetic test results** for people of African, Asian, Hispanic, or Middle Eastern ancestry.\n",
    "   - **Real-World Impact**: AI models built on such databases are likely to be more beneficial for European ancestry, creating challenges in implementing genetic solutions across diverse populations.\n",
    "\n",
    "3. **AI in Dermatology and Skin Color Bias**:\n",
    "   - **Skin Cancer Detection**: AI-driven dermatology solutions aim to improve skin cancer detection but are mostly trained on fair-skinned populations.\n",
    "   - **Disparity in Outcomes**: Patients with **darker skin**, who already face higher mortality rates from advanced skin diseases, may not benefit from AI solutions due to lack of inclusion in the training data.\n",
    "   - **Key Issues**: The AI models may struggle with diagnosing lesions in patients of color, leading to potential **misdiagnosis** or delayed diagnosis.\n",
    "\n",
    "#### Key Points for Study:\n",
    "- **Heart Attack Symptom Bias**: Gender differences in heart attack symptoms leading to biased AI predictions.\n",
    "- **Genomic Database Representation**: The heavy bias towards European genomic data and its impact on accuracy for non-European populations.\n",
    "- **Skin Lesion Detection Bias**: AI models in dermatology may fail to diagnose skin cancer in people with darker skin due to biased training data.\n",
    "- **Broader Implications**: These examples illustrate how **data underrepresentation** in AI training can lead to **systemic bias** in healthcare, negatively affecting diverse populations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction - Types of Bias\n",
    "\n",
    "#### Key Concepts:\n",
    "Bias in AI can occur at multiple stages, from **data collection** to **model development** and **deployment**. A common phrase, **\"garbage in, garbage out\"**, reflects how poor data quality or representation leads to biased models.\n",
    "\n",
    "#### Types of Bias:\n",
    "\n",
    "1. **Historical Bias**:\n",
    "   - **Definition**: Bias that exists in the data due to historical inequalities, norms, or prejudices.\n",
    "   - **Phase**: Arises during **data collection**. \n",
    "   - **Example**: Using historical healthcare data that reflects past discriminatory practices.\n",
    "\n",
    "2. **Representation Bias**:\n",
    "   - **Definition**: Occurs when certain groups are underrepresented in the dataset.\n",
    "   - **Phase**: Occurs in **data collection** and **sampling**.\n",
    "   - **Example**: AI models trained on datasets that primarily consist of data from certain racial or gender groups, leading to poor performance for underrepresented groups.\n",
    "\n",
    "3. **Measurement Bias**:\n",
    "   - **Definition**: Happens when the tools or metrics used to collect data are biased.\n",
    "   - **Phase**: Arises during **data collection** and **feature engineering**.\n",
    "   - **Example**: Biased medical instruments that produce different results based on skin color or gender.\n",
    "\n",
    "4. **Aggregation Bias**:\n",
    "   - **Definition**: Occurs when data from different groups are aggregated in ways that overlook group-specific patterns.\n",
    "   - **Phase**: Appears during **model training**.\n",
    "   - **Example**: An AI model that combines data from multiple subgroups (e.g., different races or genders) without accounting for group differences, leading to poor accuracy for minority groups.\n",
    "\n",
    "5. **Evaluation Bias**:\n",
    "   - **Definition**: Happens when the performance of a model is evaluated using biased benchmarks or metrics that do not reflect diverse populations.\n",
    "   - **Phase**: Occurs during **model evaluation**.\n",
    "   - **Example**: Using a test set that lacks diversity, which makes the model seem more accurate than it actually is for underrepresented groups.\n",
    "\n",
    "6. **Deployment Bias**:\n",
    "   - **Definition**: Bias that arises when the model is used in real-world settings that differ from the training environment.\n",
    "   - **Phase**: Happens during **model deployment**.\n",
    "   - **Example**: An AI system designed for a specific hospital setting may not perform well when deployed in rural or underserved healthcare settings.\n",
    "\n",
    "#### Key Points for Study:\n",
    "- **Historical Bias**: Bias due to pre-existing societal inequalities in the data.\n",
    "- **Representation Bias**: Lack of diverse representation in training data.\n",
    "- **Measurement Bias**: Biased data collection tools or methods.\n",
    "- **Aggregation Bias**: Generalizing across diverse groups without acknowledging specific needs or differences.\n",
    "- **Evaluation Bias**: Using non-diverse evaluation metrics or datasets.\n",
    "- **Deployment Bias**: Real-world settings differ from the conditions for which the model was trained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Bias\n",
    "\n",
    "#### Key Concepts:\n",
    "**Historical bias** occurs when the present or past state of the world, influenced by societal norms or prejudices, affects a model's predictions, making them **unfair**. This type of bias is deeply rooted in **preconceived human judgments** and is difficult to eliminate, even with perfect sampling or feature selection.\n",
    "\n",
    "#### Important Points:\n",
    "\n",
    "1. **Definition**:\n",
    "   - Historical bias reflects judgments based on societal **prejudices** or norms that are encoded in **real-world data**.\n",
    "   - AI models, being data-driven, inherit these biases, making predictions unfair to certain groups.\n",
    "\n",
    "2. **Impact on Healthcare**:\n",
    "   - **Historical healthcare data** is often dominated by **male** and **white** populations, creating biased models that fail to generalize well to **diverse populations**.\n",
    "   \n",
    "3. **Examples**:\n",
    "   - **Heart Attack Symptoms**: Early AI models for heart attack triage were trained using **male-specific symptoms**. This led to models that performed poorly for women since the **difference in symptoms** between men and women was previously unknown.\n",
    "   - **Lung Cancer Screening**: Guidelines based on a study where **96%** of participants were **white** resulted in inaccurate screening for **African Americans**, who have different **smoking habits** (e.g., use of **menthol cigarettes**).\n",
    "\n",
    "4. **Broader Implications**:\n",
    "   - Historical bias in healthcare models arises from **clinical trials** and data collection processes that were **not inclusive** of diverse populations.\n",
    "   - This bias disproportionately affects groups that carry a greater **disease burden**, but the models are designed to better suit **white males** due to past data collection practices.\n",
    "\n",
    "#### Key Points for Study:\n",
    "- **Historical Bias**: Arises from societal prejudices encoded in data.\n",
    "- **Male and White Dominated Healthcare Data**: Leads to models that underperform for diverse populations.\n",
    "- **Examples of Historical Bias**: \n",
    "   - Heart attack symptoms trained on male data.\n",
    "   - Lung cancer screening guidelines based on studies with predominantly white participants.\n",
    "- **Impact on Model Generalization**: Historical bias limits the modelâ€™s **external validity**, particularly for underrepresented groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation Bias\n",
    "\n",
    "#### Key Concepts:\n",
    "**Representation bias**, also known as **sampling bias**, occurs when the **training data** used for an AI model does not reflect the **actual distribution** of the population it is intended to serve. This leads to underrepresentation of certain demographic groups, resulting in **biased predictions**.\n",
    "\n",
    "#### Important Points:\n",
    "\n",
    "1. **Definition**:\n",
    "   - Representation bias happens when **certain parts of the final population** are **underrepresented** in the **training data** used to build an AI solution.\n",
    "   - This bias directly affects the modelâ€™s ability to generalize well across diverse populations.\n",
    "\n",
    "2. **Impact on Healthcare**:\n",
    "   - Models trained with underrepresentation of certain demographic groups (e.g., minorities) may perform poorly in **real-world applications**.\n",
    "   - This can lead to **disparities** in the accuracy and effectiveness of healthcare solutions for these underrepresented populations.\n",
    "\n",
    "3. **Example of Representation Bias**:\n",
    "   - A **Stanford systematic review** analyzed machine learning models for **Type 2 diabetes** using **electronic health records**.\n",
    "   - Findings:\n",
    "     - **Only 25%** of the reviewed papers included **population demographics**.\n",
    "     - **White** and **Black** populations were adequately represented in the training data, matching their real-world prevalence.\n",
    "     - **Hispanics** were **not included** in the training data, despite their **higher prevalence** and **complication rates** for Type 2 diabetes.\n",
    "     - This omission raises concerns about how **AI models** trained without **Hispanic** representation would perform in broader populations, particularly in **nationwide healthcare systems**.\n",
    "\n",
    "4. **Broader Implications**:\n",
    "   - Representation bias can result in models that **fail** to deliver equitable healthcare outcomes for **underrepresented groups**.\n",
    "   - Addressing this bias is critical for ensuring **fairness** and **generalizability** of AI healthcare models across **diverse populations**.\n",
    "\n",
    "#### Key Points for Study:\n",
    "- **Representation Bias**: Occurs when training data does not reflect the full population.\n",
    "- **Underrepresentation**: Leads to poor model performance for certain groups.\n",
    "- **Stanford Example**: Type 2 diabetes models lacked **Hispanic** representation, despite their high prevalence of the disease.\n",
    "- **Impact on AI Models**: Affects the **generalization** of AI solutions across nationwide systems and diverse populations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurement Bias\n",
    "\n",
    "#### Key Concepts:\n",
    "**Measurement bias** arises when **noise** in the features or labels is **unevenly distributed** across different groups, leading to **differential performance** and skewed predictions. This occurs when the **available data** is a **noisy proxy** for the actual variables of interest, and the **measurement process** introduces bias.\n",
    "\n",
    "#### Important Points:\n",
    "\n",
    "1. **Definition**:\n",
    "   - **Measurement bias** occurs when the **inaccuracy** in measuring a variable differs between groups.\n",
    "   - It results in **differential outcomes** for various populations, affecting the fairness of the AI model.\n",
    "\n",
    "2. **Impact in Healthcare**:\n",
    "   - Measurement bias can lead to **incorrect predictions** that disproportionately affect certain groups, leading to **inequitable allocation of resources** and **misdiagnosis**.\n",
    "\n",
    "3. **Example: Obermeyer Study**:\n",
    "   - Dr. Obermeyer and colleagues studied a widely used algorithm for **predicting patients** in need of **additional care support**.\n",
    "   - They found **racial bias**:\n",
    "     - The algorithm required **Black patients** to be **sicker with more co-morbidities** to achieve the same predictive score as **White patients** for accessing **beneficial care programs**.\n",
    "     - This was due to the algorithm **using healthcare costs** as a **proxy** for healthcare need.\n",
    "   - The bias was introduced because **healthcare costs** are not a reliable measure of **medical illness**, leading to **inaccurate predictions** and **unequal resource allocation**.\n",
    "\n",
    "4. **Causes**:\n",
    "   - **Noisy proxies**: When the available features (like healthcare costs) are only a **noisy proxy** for the real variables of interest (like illness severity).\n",
    "   - **Measurement errors**: These errors are often not random but reflect **systemic biases** in how different groups are measured, further **exacerbating inequalities**.\n",
    "\n",
    "5. **Mitigation Strategies**:\n",
    "   - **Awareness** of measurement bias is critical to identify and address the issue.\n",
    "   - Implementing **pre- and post-processing** actions can help **mitigate the bias**:\n",
    "     - **Pre-processing**: Correcting data before training the model.\n",
    "     - **Post-processing**: Adjusting model predictions after they are made to ensure fairness.\n",
    "\n",
    "#### Key Points for Study:\n",
    "- **Measurement Bias**: Occurs when the noise in data features or labels is not evenly distributed across groups, leading to biased predictions.\n",
    "- **Obermeyer Study**: The algorithm used **healthcare costs** as a flawed **proxy** for healthcare needs, leading to **racial bias** in care support allocation.\n",
    "- **Impact**: Black patients were required to be **sicker** to achieve the same scores as White patients, affecting healthcare resource distribution.\n",
    "- **Mitigation**: Awareness and **pre- and post-processing** strategies can help reduce the impact of measurement bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation Bias\n",
    "\n",
    "#### Key Concepts:\n",
    "**Aggregation bias** arises when **data from distinct populations** with differing underlying distributions is combined to develop a **single model**, leading to inaccurate predictions. This occurs because a **\"one-size-fits-all\" approach** fails to account for **systematic differences** between groups, requiring either **separate models** or the inclusion of demographic variables.\n",
    "\n",
    "#### Important Points:\n",
    "\n",
    "1. **Definition**:\n",
    "   - **Aggregation bias** occurs when **different populations** with **varying distributions** of the outcome under study are **aggregated** into one model.\n",
    "   - This issue is known as **infra-marginality**, where differences between groups are **not properly accounted for**, leading to **inaccurate predictions**.\n",
    "\n",
    "2. **Problem in Model Development**:\n",
    "   - Healthcare data often contains **distinct stratifications** based on factors like **ethnicity, age, gender**, etc.\n",
    "   - Developing a **single model** for all groups can result in **biased outcomes**, as certain groups might have **different base rates** for diseases or conditions.\n",
    "\n",
    "3. **Example: Diabetes and Ethnic Variations**:\n",
    "   - **Type 2 diabetes** has **differential prevalence** and **complication rates** across **racial and ethnic groups**.\n",
    "     - **US Hispanics** have higher rates (~17%) compared to **non-Hispanic Whites** (~8%).\n",
    "     - Within the Hispanic community, different subgroups (e.g., Puerto Rican vs. South American) have **varying rates** of diabetes.\n",
    "   - A model that **does not account** for these differences could produce **inaccurate predictions** or lead to **inequitable healthcare** for certain subgroups.\n",
    "\n",
    "4. **Mitigating Aggregation Bias**:\n",
    "   - **Developing separate models** for distinct populations can help improve accuracy.\n",
    "   - Alternatively, **demographic variables** (e.g., ethnicity) should be included in the model to account for these **systematic differences**.\n",
    "   - Without such adjustments, models may disproportionately **benefit or harm certain groups**, leading to **misdiagnosis** or **inequitable treatment**.\n",
    "\n",
    "5. **Gender Bias Example**:\n",
    "   - Similar to ethnic variation in diabetes, **gender bias** in heart attack symptoms showed that **AI solutions** worked well for **men** but were **inaccurate for women**.\n",
    "   - This highlights the need for **tailored models** to address **gender-specific differences**.\n",
    "\n",
    "#### Key Points for Study:\n",
    "- **Aggregation Bias**: Occurs when **data from different populations** is combined without accounting for differences, leading to **infra-marginality**.\n",
    "- **Type 2 Diabetes Example**: US **Hispanics** have **higher rates** of diabetes, but these rates vary between subgroups (e.g., Puerto Rican vs. South American).\n",
    "- **Mitigation**: Use **separate models** or include **demographic variables** to address systematic differences across populations.\n",
    "- **Gender Bias in Healthcare**: Similar issues can arise when models **overfit** to one group (e.g., men) while being **inaccurate for others** (e.g., women).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Bias\n",
    "\n",
    "#### Key Concepts:\n",
    "**Evaluation bias** occurs during the **validation and tuning** of AI models when the **testing data** or **performance metrics** used are **not representative** of the population or the problem the model is intended to address. It can lead to **misleading conclusions** about the model's real-world performance.\n",
    "\n",
    "#### Important Points:\n",
    "\n",
    "1. **Testing Data and External Benchmarks**:\n",
    "   - **Evaluation bias** can arise if the **testing data** used to validate the model does not represent the **target population**.\n",
    "   - Developers often use **benchmark** or **synthetic datasets** for model training, which may not align with **real-world conditions**.\n",
    "   - This mismatch leads to models that perform well on test data but poorly in the **real-world deployment**.\n",
    "   - **Solution**: Use **external validation** with **unseen data** drawn from the **target population** to ensure the model generalizes effectively.\n",
    "\n",
    "2. **Performance Metrics**:\n",
    "   - Inappropriate or **incomplete metrics** can also introduce **evaluation bias**.\n",
    "   - For example, using only **Area Under the Curve (AUC)** in a dataset with **class imbalance** might **overestimate performance** if **other metrics** such as **precision, recall, or F1 score** are not considered.\n",
    "   - Similarly, focusing only on overall performance may hide issues with **specific subgroups**.\n",
    "\n",
    "3. **Granular and Comprehensive Metrics**:\n",
    "   - To avoid **evaluation bias**, models should be assessed using a **range of metrics** that capture the modelâ€™s performance across all important aspects.\n",
    "   - Metrics should also be **granular** enough to measure performance across **different subgroups** (e.g., by gender, age, ethnicity) to ensure that the model **performs equitably**.\n",
    "\n",
    "4. **Example**:\n",
    "   - If a model is developed for a **medical condition** but the test data predominantly represents a specific group (e.g., **men**), the model might **underperform** when deployed on a **different group** (e.g., **women**).\n",
    "   - In cases of **class imbalance**, using **only AUC** might mask poor performance in predicting **rare events** (e.g., disease outbreaks or fraud).\n",
    "\n",
    "#### Key Points for Study:\n",
    "- **Evaluation Bias**: Can arise from **non-representative test data** or using **inappropriate metrics**.\n",
    "- **External Validation**: Models should be tested on **unseen, real-world datasets** from the target population to ensure **generalizability**.\n",
    "- **Performance Metrics**: Use a **comprehensive set of metrics** that address **class imbalance** and **subgroup-specific performance**.\n",
    "- **Mitigation**: Evaluate models **granularly** across **different subgroups** to avoid overlooking bias and ensure **fair outcomes**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment Bias\n",
    "\n",
    "#### Key Concepts:\n",
    "**Deployment bias** occurs when an AI model is **used inappropriately** or its **results are misinterpreted**, leading to a mismatch between the **model's intended use** and its **actual application**. This type of bias typically arises during the **implementation** phase of AI systems.\n",
    "\n",
    "#### Important Points:\n",
    "\n",
    "1. **Misapplication of Model Predictions**:\n",
    "   - Deployment bias happens when the **intended purpose** of the model is different from how it is actually **deployed** or used in practice.\n",
    "   - For instance, a model developed to predict **costs** may be incorrectly used to predict **healthcare needs**, resulting in **inaccurate decision-making**.\n",
    "\n",
    "2. **Example: Healthcare Cost vs. Need**:\n",
    "   - In the **Obermeyer study**, a model that accurately predicted **healthcare costs** was misused to predict **healthcare needs**.\n",
    "   - This misapplication of the model led to **racial disparities** in healthcare access, where **black patients** had to be nearly **twice as sick** as **white patients** to qualify for the same care programs.\n",
    "   - The deployment bias here resulted from assuming that **higher costs** were directly correlated with **higher healthcare need**, which was not the model's original intent.\n",
    "\n",
    "3. **Impact on Healthcare**:\n",
    "   - **Deployment bias** is particularly dangerous in healthcare, where **misinterpretation** of an AI modelâ€™s results can lead to **inequitable treatment** of patients.\n",
    "   - This can reinforce **existing societal inequalities**, especially when decisions impact **access to care**, **resource allocation**, or **treatment recommendations**.\n",
    "\n",
    "4. **Intersection of AI and Society**:\n",
    "   - Deployment bias occurs at the **intersection of AI** and **society**, highlighting how **societal understanding** and the **medical community's** use of AI can influence outcomes.\n",
    "   - A **well-developed model** can still lead to biased results if it is **misused** during deployment.\n",
    "\n",
    "#### Key Points for Study:\n",
    "- **Deployment Bias**: Occurs when an AI model is **misused** or its **results** are **misinterpreted**.\n",
    "- **Example**: In the **Obermeyer study**, a model predicting **healthcare costs** was used to predict **healthcare needs**, leading to **inequitable care**.\n",
    "- **Impact**: Can lead to **unfair outcomes** in **resource allocation** and **patient treatment**.\n",
    "- **Mitigation**: Ensuring that models are **deployed appropriately** for their **intended purpose** is critical to preventing deployment bias, especially in sensitive domains like **healthcare**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmic Fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Algorithmic Fairness?\n",
    "\n",
    "#### Key Concepts:\n",
    "**Algorithmic fairness** addresses the ethical implications of AI systems in healthcare, emphasizing the importance of **justice** and **equity** in health outcomes rather than merely focusing on algorithmic outputs. The historical context of medicine often reflects **white normativity**, which has influenced clinical practices and research inclusivity.\n",
    "\n",
    "#### Importance of Diversity in Healthcare:\n",
    "- Traditional clinical signs, such as **blue lips** as an indicator of **oxygen deprivation**, highlight biases in medical education. This sign is predominantly observable in individuals with **light skin**, demonstrating a lack of consideration for patients with **darker skin tones**.\n",
    "- The concept of **epistemic privilege** emphasizes that individuals from diverse backgrounds possess knowledge and experiences that can improve the accuracy and comprehensiveness of medical knowledge and AI models.\n",
    "\n",
    "### Classes of Algorithmic Fairness:\n",
    "\n",
    "1. **Anti-classification**:\n",
    "   - This approach aims to **exclude sensitive attributes** (such as race or gender) from the decision-making process to prevent discrimination.\n",
    "   - The underlying assumption is that if these attributes are not considered, then the resulting algorithm will be fair.\n",
    "   - **Limitations**: This method can overlook historical injustices and systemic biases inherent in the data, leading to **unintended discriminatory outcomes**.\n",
    "\n",
    "2. **Classification Parity**:\n",
    "   - This principle focuses on achieving **equal outcomes across different groups** (e.g., equal true positive rates or false positive rates).\n",
    "   - The goal is to ensure that the algorithm performs similarly for all demographic groups.\n",
    "   - **Limitations**: While it addresses disparity in outcomes, it does not necessarily account for the underlying **causes of disparity** or the contextual differences among groups.\n",
    "\n",
    "3. **Calibration**:\n",
    "   - Calibration ensures that the predicted probabilities of outcomes are **accurate across different groups**.\n",
    "   - For instance, if an algorithm predicts a 70% probability of an event for two different groups, it should hold true that 70% of individuals in both groups actually experience the event.\n",
    "   - **Limitations**: Calibration can be challenging in practice and may not adequately address **root causes of bias** or the broader social implications of its predictions.\n",
    "\n",
    "### Challenges and Considerations:\n",
    "- There is **no universal definition** of fairness, and each framework has its **shortcomings**.\n",
    "- Researchers must be **cautious** and **aware** of these limitations when applying fairness definitions in **model evaluation**.\n",
    "- The conversation around algorithmic fairness must involve **diverse perspectives** to enhance the quality of scientific inquiry and ensure equitable healthcare outcomes.\n",
    "\n",
    "#### Key Points for Study:\n",
    "- **Algorithmic Fairness**: A critical aspect of AI in healthcare focusing on **justice** and **equity**.\n",
    "- **Diversity Importance**: Lack of representation can lead to biased clinical signs and algorithms.\n",
    "- **Three Forms of Fairness**: \n",
    "  - **Anti-classification**: Excludes sensitive attributes.\n",
    "  - **Classification Parity**: Ensures equal outcomes for different groups.\n",
    "  - **Calibration**: Ensures accurate predicted probabilities across groups.\n",
    "- **Limitations**: Each fairness concept has statistical shortcomings and requires careful application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anti-classification\n",
    "\n",
    "#### Definition of Anti-classification:\n",
    "**Anti-classification**â€”or **fairness through unawareness**â€”is a framework that posits the exclusion of protected attributes (such as gender, race, or ethnicity) from outcome modeling in AI algorithms. This approach aims to prevent discrimination by ensuring that sensitive characteristics do not influence the model's decisions.\n",
    "\n",
    "#### Key Concepts:\n",
    "- **Protected Attributes**: Characteristics like gender and race that are safeguarded by anti-discrimination laws. The strict interpretation of anti-classification suggests omitting any unprotected characteristics that might act as proxies for these protected attributes.\n",
    "- **Proxies**: Features that may indirectly correlate with protected attributes (e.g., attendance at a single-sex school, height, or weight), making the identification of true proxies challenging without further data collection.\n",
    "\n",
    "#### Challenges of Anti-classification:\n",
    "1. **Practicality Issues**:\n",
    "   - Removing all potential proxies may significantly reduce the number of useful features, making it difficult to develop effective predictive models.\n",
    "   - Identifying which variables are true proxies for protected attributes requires thorough analysis, which may not always be feasible.\n",
    "\n",
    "2. **Indirect Bias**:\n",
    "   - Even if an algorithm does not directly consider protected attributes, it can still produce biased outcomes based on proxies. This means the exclusion of sensitive attributes does not guarantee fairness.\n",
    "\n",
    "3. **Infra-marginality Problem**:\n",
    "   - Some clinical risk models need to include protected attributes to ensure equitable outcomes, particularly when the risk distributions differ across sub-populations.\n",
    "   - For instance, diabetes diagnosis and monitoring protocols often require different approaches for various ethnicities and genders, necessitating the inclusion of these attributes in models.\n",
    "\n",
    "#### Strategies for Mitigating Bias:\n",
    "- **Focus on Reducing Disparities**:\n",
    "  - Developers should aim to reduce differences in model performance between groups without sacrificing overall effectiveness. Identifying and addressing trade-offs is crucial in this process.\n",
    "\n",
    "- **Enhancing Dataset Representation**:\n",
    "  - AI solutions trained on datasets that underrepresent certain groups may require additional training data. This can help improve accuracy and fairness in decision-making, ensuring that algorithms do not yield unfair results.\n",
    "\n",
    "- **Equitable Model Design**:\n",
    "  - When aiming for fairness, models must learn to leverage protected attributes judiciously to avoid reinforcing existing inequalities while still addressing the nuances of different populations.\n",
    "\n",
    "### Conclusion:\n",
    "Anti-classification as a fairness framework offers a starting point for addressing bias in AI algorithms; however, its practical limitations and challenges highlight the complexity of achieving true fairness in healthcare applications. Developers must balance the exclusion of sensitive attributes with the need for comprehensive models that account for the diverse experiences of patients. Ultimately, a nuanced approach that combines equitable modeling practices with robust data representation strategies is essential for mitigating potential biases in AI healthcare solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parity Classification\n",
    "\n",
    "#### Definition of Classification Parity:\n",
    "**Classification parity** is a fairness framework that advocates for equal predictive performance across different protected groups. This means that the performance metrics of a model should not vary based on sensitive attributes such as race, gender, or ethnicity. The focus is on ensuring that predictions made by the model are equally accurate across all demographic groups.\n",
    "\n",
    "#### Key Concepts:\n",
    "- **Relevant Performance Metrics**: The selection of performance metrics should align with the actionable insights derived from the model outputs. Depending on the specific problem at hand, certain metrics may be more pertinent than others.\n",
    "\n",
    "- **Critical Metrics in Classification Parity**:\n",
    "  1. **False Positive Rate (FPR)**: This refers to the probability of incorrectly predicting a positive outcome when the true outcome is negative. Classification parity requires that the FPR remains consistent across all demographic groups.\n",
    "  2. **False Negative Rate (FNR)**: In some contexts, the FNR, which indicates the probability of failing to predict a positive outcome when it is true, may be more significant to consider, depending on the nature of the application.\n",
    "  3. **Proportion of Positive Decisions**: Also known as **demographic parity**, this measure states that the probability of predicting a positive outcome should be uniform across different demographic groups when controlling for other variables. \n",
    "\n",
    "#### Challenges of Classification Parity:\n",
    "- **Infra-marginality Issue**: \n",
    "  - The definitions of classification parity may not be applicable when the underlying risk distributions differ among groups. This is particularly relevant in healthcare, where different populations may exhibit varying risk profiles.\n",
    "  - When risk distributions are different, enforcing classification parity can lead to misleading conclusions or ineffective interventions, as the needs and characteristics of different groups are not adequately addressed.\n",
    "\n",
    "- **Threshold-based Decision Making**: \n",
    "  - Classification models often rely on threshold-based decisions, which can produce error metrics that differ across demographic groups. This discrepancy can complicate the achievement of true classification parity, as it may lead to unequal outcomes.\n",
    "\n",
    "#### Implications in Healthcare:\n",
    "- **Clinical Relevance**: In the medical field, the assumption of equal predictive performance is crucial for ensuring equitable healthcare delivery. Disparities in predictive performance could result in unequal access to care or misdiagnosis across different demographic groups.\n",
    "  \n",
    "- **Need for Tailored Approaches**: Given the complexities associated with differing risk distributions, it is vital for developers to consider tailored modeling approaches that incorporate the unique characteristics of various populations while striving for fairness.\n",
    "\n",
    "### Conclusion:\n",
    "Classification parity serves as a valuable framework for evaluating fairness in AI models, emphasizing the importance of equal predictive performance across protected groups. However, challenges such as infra-marginality and the implications of threshold-based decision-making necessitate careful consideration and adaptation in healthcare applications. To promote equitable outcomes, developers must balance the principles of classification parity with the realities of diverse patient populations and their specific needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration\n",
    "\n",
    "#### Definition of Calibration:\n",
    "**Calibration** refers to the alignment between predicted probabilities and observed outcomes in a predictive model. A well-calibrated model ensures that, for a given risk score, the proportion of actual positive outcomes aligns with the predicted probabilities. For instance, if a model predicts a risk score of 25, we would expect approximately 25 out of 100 patients with that score to experience the event of interest.\n",
    "\n",
    "#### Calibration in Fairness Analysis:\n",
    "In the context of algorithmic fairness, calibration emphasizes that outcomes should be independent of protected attributes when conditioning on risk estimates. This means that a model should accurately reflect probabilities for all demographic groups without bias. Key points include:\n",
    "\n",
    "- **Risk Prediction and Outcome Agreement**: Calibration seeks to ensure that the predicted risk scores correspond closely with the actual probabilities of outcomes across different demographic groups.\n",
    "\n",
    "- **Comparison of Outputs**: Calibration can be viewed as a comparison between the actual outcomes and the expected probabilities produced by the model. The goal is to refine the model such that the predicted risk distribution mirrors the observed outcomes in the training data.\n",
    "\n",
    "#### Challenges with Calibration:\n",
    "- **Manipulation of Risk Distributions**: Calibration can be susceptible to manipulation, where changes in data aggregation or feature representation can skew the outcomes. For example, modifying how data is grouped or which features are included can lead to misleading trends in predictions.\n",
    "\n",
    "- **Consequences of Poor Calibration**: If an AI model is poorly calibrated, it can result in false expectations for users, whether patients or healthcare professionals. This misalignment can lead to poor decision-making, such as inappropriate treatment plans or mismanagement of care.\n",
    "\n",
    "#### Clinical Implications:\n",
    "- **In Vitro Fertilization (IVF) Example**: In the context of IVF treatment, accurate calibration is critical. An overestimation of the likelihood of live birth could provide false hope to couples, potentially leading to emotional distress and exposing women to unnecessary medical risks, such as ovarian hyperstimulation syndrome.\n",
    "\n",
    "- **Importance of Calibration Evaluation**: Ensuring proper calibration is vital before deploying models in clinical settings. Poorly calibrated models can lead to misguided expectations and decisions, highlighting the need for rigorous evaluation during the development phase.\n",
    "\n",
    "### Conclusion:\n",
    "Calibration is a fundamental aspect of algorithmic fairness, ensuring that predictive models deliver accurate risk assessments that align with observed outcomes across diverse demographic groups. The importance of calibration is underscored by its direct implications for patient care and decision-making in healthcare settings. To maintain ethical standards in AI applications, developers must prioritize calibration and thoroughly assess models before deployment to prevent adverse consequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Fairness Measures\n",
    "\n",
    "#### Introduction\n",
    "Algorithmic fairness concepts, including **anti-classification**, **classification parity**, and **calibration**, can be applied to assess whether weather model predictions are equitable across different demographic groups. This exploration highlights how these principles can enhance the integrity of predictive models in various contexts, including weather forecasting.\n",
    "\n",
    "#### 1. Anti-Classification\n",
    "- **Definition**: Anti-classification requires the exclusion of protected attributes (e.g., race, gender) in model decision rules. A stricter interpretation prohibits the use of proxies for these attributes, such as geographic indicators (e.g., zip codes) that correlate with race.\n",
    "\n",
    "- **Implementation**: \n",
    "  - Evaluating anti-classification is straightforward when excluding protected attributes. However, identifying and excluding proxies can be challenging due to subtle correlations.\n",
    "  - Once relevant performance metrics are selected, differences in model performance across demographic groups can be assessed using a test set. \n",
    "  - **Empirical Confidence Intervals**: It is advisable to calculate empirical confidence intervals through methods like bootstrapping to minimize dependence on the test set. Non-overlapping confidence intervals indicate statistically significant performance differences among groups.\n",
    "\n",
    "#### 2. Classification Parity\n",
    "- **Definition**: Classification parity involves ensuring equal predictive performance across different demographic groups, often assessed through metrics like false positive and negative rates.\n",
    "\n",
    "- **Implementation**: \n",
    "  - Select key performance metrics to analyze disparities across groups.\n",
    "  - Analyze the equality of predictive performance to confirm whether the model treats all demographic groups fairly.\n",
    "\n",
    "#### 3. Calibration\n",
    "- **Definition**: Calibration measures the agreement between predicted probabilities and observed outcomes, ensuring that predictions accurately reflect the likelihood of events across demographic groups.\n",
    "\n",
    "- **Implementation**: \n",
    "  - Assess calibration by examining overall predicted and observed outcomes across different demographic groups.\n",
    "  - **Bootstrapping** can be utilized to verify the consistency of observed disparities between prediction and outcome rates, revealing systematic under- or overestimations of risk.\n",
    "\n",
    "- **Challenges**:\n",
    "  - Calibration is particularly complex for binary predictions linked to continuous risk functions. To analyze calibration, patients must be grouped by risk, allowing for a comparison of observed incidence rates to predicted risks.\n",
    "  - The choice of grouping methodologyâ€”whether by risk score, percentiles, or decilesâ€”can significantly affect the calibration analysis. A common approach is to create ten groups (deciles) and compute a chi-square statistic.\n",
    "\n",
    "- **Visualization**: \n",
    "  - The calibration curve can be plotted, with observed rates against predicted risks. A well-calibrated model will show points close to the diagonal line. If the calibration curves differ significantly across demographic groups, it indicates a violation of fairness criteria.\n",
    "\n",
    "#### Conclusion\n",
    "In summary, the three fairness conceptsâ€”anti-classification, classification parity, and calibrationâ€”provide frameworks for evaluating weather model predictions in terms of equity across demographic groups. By applying these principles, developers and researchers can enhance the fairness and effectiveness of AI solutions, ultimately leading to more equitable outcomes in weather forecasting and beyond."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transparency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lack of Transparency\n",
    "\n",
    "#### Introduction\n",
    "The use of AI solutions in healthcare, particularly those trained on electronic health records (EHRs), raises concerns about potential discrimination, especially if these models are based on historic or retrospective data. A critical aspect of addressing these concerns is ensuring that the populations represented in EHR systems are reflective of the broader community.\n",
    "\n",
    "#### Population Representativeness\n",
    "- **Concerns**: Many AI solutions are developed using data from academic medical centers, which may not accurately represent the diversity of the broader patient population. If the training data lacks diversity, the effectiveness and applicability of AI models for various patient groups become questionable.\n",
    "\n",
    "#### Transparency and Reporting\n",
    "- **Importance of Reporting**: Transparency in reporting demographic breakdowns of training data is essential for evaluating the bias and fairness of AI solutions. Without this information, stakeholdersâ€”including healthcare professionals, patients, and insurersâ€”cannot understand or mitigate potential biases inherent in the models.\n",
    "\n",
    "- **Findings from Research**:\n",
    "  - A study conducted by the authorâ€™s team assessed the reporting and demographics of machine learning models developed using EHRs.\n",
    "  - Results revealed significant inconsistencies in the reporting of demographic variables:\n",
    "    - Race and ethnicity were omitted in 64% of articles.\n",
    "    - Gender and age were not reported in 75% of studies.\n",
    "    - Socioeconomic status was missing in over 90% of the studies.\n",
    "  - Even in studies that reported these variables, it was often unclear whether they were used as model inputs.\n",
    "  - The demographic profiles of the populations included in these studies showed a higher proportion of White and Black individuals compared to fewer Hispanics, highlighting potential biases in the training data.\n",
    "\n",
    "#### Implications for AI Deployment\n",
    "- **Need for Detailed Information**: To ensure the unbiased deployment of AI models in healthcare, detailed information about the data used in model development and training is crucial. Without this transparency, it is challenging to understand the potential limitations and biases of the models.\n",
    "\n",
    "#### Conclusion\n",
    "The findings emphasize a significant gap in the transparency and representativeness of training data used in AI solutions for healthcare. Addressing these issues is vital for fostering trust and ensuring equitable outcomes when implementing AI technologies in clinical settings. Enhanced reporting practices and demographic inclusivity are necessary steps to improve the applicability and fairness of AI solutions across diverse patient populations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal Reporting Standards\n",
    "\n",
    "#### Introduction\n",
    "To promote transparency and identify best practices for designing machine learning models that account for biases and fairness, we propose the MINIMAR template (Minimum Information for Medical AI Reporting). This checklist aims to assist researchers in the thorough reporting of AI solutions in healthcare.\n",
    "\n",
    "#### MINIMAR Requirements\n",
    "1. **Population Information**:\n",
    "   - Include details about the data sources and cohort selection that provided the training data.\n",
    "   \n",
    "2. **Training Data Demographics**:\n",
    "   - Provide demographic information on the training data, enabling comparisons with the population to which the model will be applied.\n",
    "   \n",
    "3. **Model Architecture and Development**:\n",
    "   - Offer comprehensive details about the model architecture and its development, facilitating interpretation of the modelâ€™s intent and enabling replication.\n",
    "   \n",
    "4. **Model Evaluation, Optimization, and Validation**:\n",
    "   - Report model evaluation, optimization, and validation processes transparently to clarify how local model optimization can be achieved and to support replication and resource sharing.\n",
    "\n",
    "#### Example Study Application\n",
    "To illustrate the application of the MINIMAR criteria, we present a study from our lab focusing on predicting postoperative pain in patients taking certain antidepressants and prodrug opioids.\n",
    "\n",
    "- **Study Overview**:\n",
    "  - **Hypothesis**: Certain antidepressants inhibit the activation of common prodrug opioids.\n",
    "  - **Target Population**: Patients undergoing elective surgery.\n",
    "  - **Data Source**: Electronic Health Records (EHRs) from an academic hospital.\n",
    "  - **Cohort Selection**: Included only adult patients, excluding those who died during hospitalization.\n",
    "\n",
    "- **Demographic Reporting**:\n",
    "  - **Variables**: Age, gender, race, and ethnicity breakdown reported.\n",
    "  - **Proxy for Socioeconomic Status**: Insurance types were used as a proxy.\n",
    "\n",
    "- **Model Output**:\n",
    "  - **Target**: Predicting pain scores to aid healthcare workers in triaging patients at risk of uncontrolled pain post-surgery.\n",
    "\n",
    "- **Model Development**:\n",
    "  - **Train-Test Split**: Utilized tenfold data splitting.\n",
    "  - **Gold Standard**: 100 manually annotated notes, alongside structured pain scores recorded in the EHRs.\n",
    "  - **Model Task**: Regression model for prediction.\n",
    "  - **Features**: Detailed reporting of features used, transformations applied, and methods for handling missing data.\n",
    "\n",
    "- **Model Optimization and Validation**:\n",
    "  - Provided parameters for model optimization and validation details.\n",
    "  - Code and de-identified data shared via GitHub for transparency.\n",
    "\n",
    "#### Transparency Gaps\n",
    "While the study met many MINIMAR criteria, a notable gap was the lack of external validation using unseen data separate from the training set. This limitation prevents verification of the model's generalizability.\n",
    "\n",
    "#### Conclusion\n",
    "The MINIMAR template provides a structured approach to ensure transparency in reporting AI solutions in healthcare. By adhering to these guidelines, researchers can enhance the understanding of model deployment and applicability across diverse patient populations. The example study demonstrates the potential benefits of thorough reporting, while also highlighting areas where improvements are needed, such as external validation, to establish model generalizability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downstream Evalautions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opportunities and Challenges\n",
    "\n",
    "#### Introduction\n",
    "The lecture addressed the complexities surrounding bias and algorithmic fairness in AI, emphasizing the ongoing challenges and opportunities in this evolving field. As researchers navigate the intricacies of AI fairness, a deeper understanding of its definitions, implications, and potential pathways for improvement is essential.\n",
    "\n",
    "#### Challenges in Defining Fairness\n",
    "1. **Varied Definitions**: \n",
    "   - Numerous definitions of AI fairness exist in the literature, each reflecting different use cases and perspectives. This diversity complicates the identification of a single, universally applicable fairness solution.\n",
    "   - The debate continues around the best definition of fairness, ranging from **quality** to **equity**.\n",
    "\n",
    "2. **Equality vs. Equity**:\n",
    "   - **Equality**: Providing each individual or group with the same resources, attention, or outcomes. This concept is illustrated by examples like the Obermaier case.\n",
    "   - **Equity**: Ensuring that groups receive the resources necessary to achieve similar outcomes, highlighting the importance of tailoring support based on individual needs.\n",
    "\n",
    "3. **Complex Implementation**: \n",
    "   - Developing models that balance both equality and equity requires a paradigm shift in healthcare delivery, presenting a significant area of ongoing research focused on identifying and mitigating biases in datasets.\n",
    "\n",
    "4. **Systematic Biases**:\n",
    "   - Many biases in AI are systemic and often go unnoticed, complicating efforts to address them. For instance, the blue lips example illustrates how biases can manifest in unexpected ways.\n",
    "   - While biases can be identified, systematically mitigating them remains a significant challenge, necessitating the development of tools for healthcare professionals to address biases at the point of care.\n",
    "\n",
    "#### Opportunities for Advancement\n",
    "1. **Growing Research and Awareness**: \n",
    "   - The increasing number of publications on bias in healthcare AI indicates a proactive movement towards creating fair and unbiased data-driven healthcare solutions.\n",
    "   - This growing body of work reflects a collective commitment to addressing bias and enhancing fairness in AI systems.\n",
    "\n",
    "2. **Transparency and Accountability**:\n",
    "   - Collecting and reporting AI outputs, clinical recommendations, and patient decisionsâ€”alongside eventual outcomesâ€”are crucial for accountability within healthcare institutions and among clinicians.\n",
    "   - Transparency is particularly vital for populations historically distrustful of the medical establishment, especially in contexts involving AI.\n",
    "\n",
    "3. **Benefit for All Groups**:\n",
    "   - To leverage AI effectively, it is crucial to ensure that it benefits all demographic groups. This requires thoughtful evaluations and careful human interpretations of AI outputs.\n",
    "   - Fostering an environment of inclusivity and trust will enhance the acceptance and effectiveness of AI solutions in healthcare.\n",
    "\n",
    "#### Conclusion\n",
    "The challenges surrounding bias and algorithmic fairness in AI highlight the complexity of defining and implementing fair solutions in healthcare. However, the opportunities presented by increasing research, transparency, and a commitment to equity signal a promising direction for future advancements. As the field continues to evolve, it is essential to prioritize the development of inclusive and accountable AI systems that genuinely benefit all populations, ensuring a more equitable healthcare landscape."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
